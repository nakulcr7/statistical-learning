batch_error <- rep(0, N);
stoch_error <- rep(0, N);
lda_error <- rep(0, N);
for (i in 1:1)
{
batch_error[i] <- find_errors(batch_func(batch_learning_rate, training_data), validation_data)
stoch_error[i] <- find_errors(stochastic_func(stoch_learning_rate, training_data), validation_data)
lda_error[i] <- find_errors_lda(validation_data, lda_func(training_data, validation_data))
}
par(mfrow=c(1, 3))
hist(batch_e, xlim=c(2.4,3.6))
# Parts of below code are from Guo & Gundogdu's HW2 solution
N <- 200
# Chosen from the graphs in the previous question
batch_learning_rate <- 1.8
stoch_learning_rate  <- 1.5
batch_error <- rep(0, N);
stoch_error <- rep(0, N);
lda_error <- rep(0, N);
for (i in 1:1)
{
batch_error[i] <- find_errors(batch_func(batch_learning_rate, training_data), validation_data)
stoch_error[i] <- find_errors(stochastic_func(stoch_learning_rate, training_data), validation_data)
lda_error[i] <- find_errors_lda(validation_data, lda_func(training_data, validation_data))
}
par(mfrow=c(1, 3))
hist(batch_error, xlim=c(2.4,3.6))
hist(stoch_error, xlim=c(2.4,3.6))
hist(lda_error, xlim=c(2.4,3.6))
# Parts of below code are from Guo & Gundogdu's HW2 solution
N <- 200
# Chosen from the graphs in the previous question
batch_learning_rate <- 1.8
stoch_learning_rate  <- 1.5
batch_error <- rep(0, N);
stoch_error <- rep(0, N);
lda_error <- rep(0, N);
for (i in 1:2)
{
batch_error[i] <- find_errors(batch_func(batch_learning_rate, training_data), validation_data)
stoch_error[i] <- find_errors(stochastic_func(stoch_learning_rate, training_data), validation_data)
lda_error[i] <- find_errors_lda(validation_data, lda_func(training_data, validation_data))
}
# Batch and Stochastic Regression errors
find_errors_regression <- function(beta, validation_df){
beta = beta[-1 * length(beta)]
df_x = cbind(x0=1, validation_df[, c(1:(ncol(val_df) - 1))])
prob = 1 / (1 + exp(-1 * (as.matrix(df_x) %*% cbind(beta))))
y_pred = rbinom(50, 1, prob)
t = table(actual=validation_df$y, predict=y_pred)
correct = (t[1,1]+t[2,2]) / sum(t)
return(1 - correct)
}
# LDA errors
find_errors_lda = function(validation_data, lda_prediction){
t = table(actual = validation_data$y, predict = lda_prediction)
correct = (t[1,1]+t[2,2])/sum(t)
return(1-correct)
}
# Parts of below code are from Guo & Gundogdu's HW2 solution
N <- 200
# Chosen from the graphs in the previous question
batch_learning_rate <- 1.8
stoch_learning_rate  <- 1.5
batch_error <- rep(0, N);
stoch_error <- rep(0, N);
lda_error <- rep(0, N);
for (i in 1:2)
{
batch_error[i] <- find_errors(batch_func(batch_learning_rate, training_data), validation_data)
stoch_error[i] <- find_errors(stochastic_func(stoch_learning_rate, training_data), validation_data)
lda_error[i] <- find_errors_lda(validation_data, lda_func(training_data, validation_data))
}
par(mfrow=c(1, 3))
hist(batch_error, xlim=c(2.4,3.6))
hist(stoch_error, xlim=c(2.4,3.6))
hist(lda_error, xlim=c(2.4,3.6))
# Parts of below code are from Guo & Gundogdu's HW2 solution
N <- 200
# Chosen from the graphs in the previous question
batch_learning_rate <- 1.8
stoch_learning_rate  <- 1.5
batch_error <- rep(0, N);
stoch_error <- rep(0, N);
lda_error <- rep(0, N);
for (i in 1:N)
{
batch_error[i] <- find_errors(batch_func(batch_learning_rate, training_data), validation_data)
stoch_error[i] <- find_errors(stochastic_func(stoch_learning_rate, training_data), validation_data)
lda_error[i] <- find_errors_lda(validation_data, lda_func(training_data, validation_data))
}
# Parts of below code are from Guo & Gundogdu's HW2 solution
N <- 200
# Chosen from the graphs in the previous question
batch_learning_rate <- 1.8
stoch_learning_rate  <- 1.5
batch_error <- rep(0, N);
stoch_error <- rep(0, N);
lda_error <- rep(0, N);
for (i in 1:5)
{
batch_error[i] <- find_errors(batch_func(batch_learning_rate, training_data), validation_data)
stoch_error[i] <- find_errors(stochastic_func(stoch_learning_rate, training_data), validation_data)
lda_error[i] <- find_errors_lda(validation_data, lda_func(training_data, validation_data))
}
reg_batch_func <- function(learn_rate, train_df, lambda){
delta <- 1/1000
train_df_x <- cbind(x_0 = 1, train_df %>% dplyr::select(-y))
b <- cbind(rep(0, (ncol(train_df_x))))
b_prev <- cbind(rep(1,(ncol(train_df_x))))
b_diff <- abs(b - b_prev)
y <- train_df$y
i <- 0
while(i < 1000 && length(b_diff[b_diff > delta]) > 1) {
i <- i + 1
cost <- 1 / (1 + exp(-1 * (as.matrix(train_df_x) %*% b)))
b_prev <- b
b <- b + (learn_rate / nrow(train_df)) * (t(as.matrix(train_df_x)) %*% (y - cost) - (b * lambda))
b_diff <- abs(b - b_prev)
}
return(c(b, i))
}
# I chose values of lambda by doubling them each time.
# I saw this technique in one of Andrew Ng's Machine Learning videos
lambdas = c()
i = 0.1
while(length(lambdas) < 10){
lambdas <- c(lambdas, i)
i <- i*2
}
# Obtained from previous questions
learn_rate <- 1.8
e <- rep(0, length(lambdas))
for(i in 1:length(lambdas)){
e[i] <- find_errors(batch_reg_func(learn_rate, training_data, lambdas[i]), validation_data)
}
reg_batch_func <- function(learn_rate, train_df, lambda){
delta <- 1/1000
train_df_x <- cbind(x_0 = 1, train_df %>% dplyr::select(-y))
b <- cbind(rep(0, (ncol(train_df_x))))
b_prev <- cbind(rep(1,(ncol(train_df_x))))
b_diff <- abs(b - b_prev)
y <- train_df$y
i <- 0
while(i < 1000 && length(b_diff[b_diff > delta]) > 1) {
i <- i + 1
cost <- 1 / (1 + exp(-1 * (as.matrix(train_df_x) %*% b)))
b_prev <- b
b <- b + (learn_rate / nrow(train_df)) * (t(as.matrix(train_df_x)) %*% (y - cost) - (b * lambda))
b_diff <- abs(b - b_prev)
}
return(c(b, i))
}
# I chose values of lambda by doubling them each time.
# I saw this technique in one of Andrew Ng's Machine Learning videos
lambdas = c()
i = 0.1
while(length(lambdas) < 10){
lambdas <- c(lambdas, i)
i <- i*2
}
# Obtained from previous questions
learn_rate <- 1.8
e <- rep(0, length(lambdas))
for(i in 1:length(lambdas)){
e[i] <- find_errors(reg_batch_func(learn_rate, training_data, lambdas[i]), validation_data)
}
prob_p2_func <- function(x) {
return(1/(1 + exp(-(-3 + x))))
}
p2_func <- function(seed_value) {
set.seed(seed_value)
N <- 50
x1 <- runif(N, min=0, max=3)
x2 <- runif(N, min=0, max=3)
x3 <- 0.8 * x2 + rnorm(N, mean = 0, sd = sqrt(0.75))
x4 <- 0.8 * x2 + rnorm(N, mean = 0, sd = sqrt(0.75))
x5 <- runif(N, min=0, max=3)
x6 <- runif(N, min=0, max=3)
x7 <- runif(N, min=0, max=3)
x8 <- runif(N, min=0, max=3)
x9 <- runif(N, min=0, max=3)
x10 <- runif(N, min=0, max=3)
x11 <- runif(N, min=0, max=3)
x12 <- runif(N, min=0, max=3)
p_of_1 <- prob_p2_func(x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10 + x11 + x12)
y <- rbinom(N, 1, p_of_1)
return(data.frame(x1, x2, x3, x4, x5, x6, x7, x8, x9, x10, x11, x12, y))
}
training_data <- p2_func(123)
validation_data <- p2_func(456)
reg_batch_func <- function(learn_rate, train_df, lambda){
delta <- 1/1000
train_df_x <- cbind(x_0 = 1, train_df %>% dplyr::select(-y))
b <- cbind(rep(0, (ncol(train_df_x))))
b_prev <- cbind(rep(1,(ncol(train_df_x))))
b_diff <- abs(b - b_prev)
y <- train_df$y
i <- 0
while(i < 1000 && length(b_diff[b_diff > delta]) > 1) {
i <- i + 1
cost <- 1 / (1 + exp(-1 * (as.matrix(train_df_x) %*% b)))
b_prev <- b
b <- b + (learn_rate / nrow(train_df)) * (t(as.matrix(train_df_x)) %*% (y - cost) - (b * lambda))
b_diff <- abs(b - b_prev)
}
return(c(b, i))
}
# I chose values of lambda by doubling them each time.
# I saw this technique in one of Andrew Ng's Machine Learning videos
lambdas = c()
i = 0.1
while(length(lambdas) < 10){
lambdas <- c(lambdas, i)
i <- i*2
}
# Obtained from previous questions
learn_rate <- 1.8
e <- rep(0, length(lambdas))
for(i in 1:length(lambdas)){
e[i] <- find_errors(reg_batch_func(learn_rate, training_data, lambdas[i]), validation_data)
}
# Parts of below code are from Guo & Gundogdu's HW2 solution
N <- 200
# Chosen from the graphs in the previous question
batch_learning_rate <- 1.8
stoch_learning_rate  <- 1.5
batch_error <- rep(0, N);
stoch_error <- rep(0, N);
lda_error <- rep(0, N);
for (i in 1:5)
{
batch_error[i] <- find_errors(batch_func(batch_learning_rate, training_data), validation_data)
stoch_error[i] <- find_errors(stochastic_func(stoch_learning_rate, training_data), validation_data)
lda_error[i] <- find_errors_lda(validation_data, lda_func(training_data, validation_data))
}
batch_error[i] <- find_errors(batch_func(batch_learning_rate, training_data), validation_data)
batch_error[i] <- find_errors_regression(batch_func(batch_learning_rate, training_data), validation_data)
find_errors_regression <- function(beta, validation_df){
beta = beta[-1 * length(beta)]
df_x = cbind(x0=1, validation_df[, c(1:(ncol(validation_df) - 1))])
prob = 1 / (1 + exp(-1 * (as.matrix(df_x) %*% cbind(beta))))
y_pred = rbinom(50, 1, prob)
t = table(actual=validation_df$y, predict=y_pred)
correct = (t[1,1]+t[2,2]) / sum(t)
return(1 - correct)
}
batch_error[i] <- find_errors_regression(batch_func(batch_learning_rate, training_data), validation_data)
find_errors_regression <- function(beta, validation_df){
beta = beta[-1 * length(beta)]
df_x = cbind(x0=1, validation_df[, c(1:(ncol(validation_df) - 1))])
prob = 1 / (1 + exp(-1 * (as.matrix(df_x) %*% cbind(beta))))
y_pred = rbinom(50, 1, prob)
t = table(actual=validation_df$y, predict=y_pred)
print(t)
correct = (t[1,1]+t[2,2]) / sum(t)
return(1 - correct)
}
batch_error[i] <- find_errors_regression(batch_func(batch_learning_rate, training_data), validation_data)
head(training_data)
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(comment = '>')
# Imports
library(ggplot2)
library(dplyr)
library(tidyr)
library(readr)
set.seed(123)
prob_func_p1 <- function(x1, x2) {
return(1/(1 + exp(-(-3 + x1 + x2))))
}
p1_func <- function(seed_value) {
set.seed(seed_value)
N <- 50
x1 <- runif(N, min=0, max=3)
x2 <- runif(N, min=0, max=3)
p_of_1 <- prob_func_p1(x1, x2)
y <- rbinom(N, 1, p_of_1)
return(data.frame(x1, x2, y))
}
training_data <- p1_func(123)
validation_data <- p1_func(456)
# Logistic Regression
glm.fit <- glm(y ~ x1 + x2, data = training_data, family = binomial)
print(glm.fit)
plot(glm.fit)
# LDA
library(MASS)
lda.fit <- lda(y ~ x1 + x2, data = training_data)
print(lda.fit)
plot(lda.fit)
lda.pred <- predict(lda.fit, newdata = validation_data)
t = table(actual= validation_data$y, predict=lda.pred$class)
cat("Proportion of correctly classified observations for LDA : ", (t[1,1]+t[2,2])/sum(t)*100, "%")
# Plots
p <- ggplot(data = validation_data, aes(x = x1, y = x2)) +
geom_point(aes(color = y))
# True decision boundary
p <- p + geom_abline(slope = -1, intercept = 3, linetype = "dashed")
# Logistic Regression decision boundary
p <- p + geom_abline(slope = -logreg.coefs[2], intercept = -logreg.coefs[1], color = "red")
p
batch_func <- function(learn_rate, train_df){
delta <- 1/1000
train_df_x <- cbind(x_0 = 1, train_df %>% dplyr::select(-y))
b <- cbind(rep(0, (ncol(train_df_x))))
b_prev <- cbind(rep(1,(ncol(train_df_x))))
b_diff <- abs(b - b_prev)
y <- train_df$y
i <- 0
while(i < 1000 && length(b_diff[b_diff > delta]) > 1) {
i <- i + 1
cost <- 1 / (1 + exp(-1 * (as.matrix(train_df_x) %*% b)))
b_prev <- b
b <- b + (learn_rate / nrow(train_df)) * (t(as.matrix(train_df_x)) %*% (y-cost))
b_diff <- abs(b - b_prev)
}
return(c(b, i))
}
# The below code is from Guo & Gundogdu's solutions for HW2
alpha<-seq(0.001,4.001,0.04)
slopes<-rep(0,length(alpha))
time<-rep(0,length(alpha))
for (i in 1:length(alpha))
{
slopes[i]<- batch_func(alpha[i],training_data)[2];
time[i]<- batch_func(alpha[i],training_data)[4];
}
par(mar = c(5,5,5,5))
plot(alpha, slopes, type="b")
plot(alpha, scale(time), type="b")
stochastic_func <- function(learn_rate, train_df){
delta <- 1/1000
train_df_x <- cbind(x_0 = 1, train_df %>% dplyr::select(-y))
b <- cbind(rep(0, (ncol(train_df_x))))
b_prev <- cbind(rep(1,(ncol(train_df_x))))
b_diff <- abs(b - b_prev)
y <- train_df$y
i <- 0
while(i < 1000 && length(b_diff[b_diff > delta]) > 1) {
b_prev <- b
for(k in 1:nrow(train_df_x)){
cost <- 1 / (1 + exp(-1 * (as.matrix(train_df_x) %*% b)))
b <- b + (learn_rate / nrow(train_df)) * (t(as.matrix(train_df_x)) %*% (y[k] - cost))
}
i <- i+1
b_diff <- abs(b - b_prev)
}
return(c(b,i))
}
alpha<-seq(0.001,4.001,0.04)
slopes<-rep(0, length(alpha))
time<-rep(0, length(alpha))
for (i in 1:length(alpha))
{
slopes[i]<- stochastic_func(alpha[i], training_data)[2];
time[i]<- stochastic_func(alpha[i], training_data)[4];
}
plot(alpha,slopes,type="b")
plot(alpha,scale(time),type="b")
lda_func <- function(train_df, validation_df) {
groups <- unique(train_df$y)
mu <- data_frame()
constant <- c()
covar_inv <- solve(cov(train_df %>% dplyr::select(-y)))
for(k in groups) {
train_df_k <- train_df %>% filter(y==k) %>% dplyr::select(-y)
mu_k <- apply(train_df_k, 2, mean)
mu <- mu %>% bind_rows(data.frame(as.list(mu_k)))
pi_k <-nrow(train_df_k) / nrow(train_df)
constant_k <- (0.5 * t(mu_k) %*% covar_inv %*% matrix(mu_k)) + log(pi_k)
constant <- c(constant, constant_k)
}
lda_model <- data.frame(groups = groups, constant = constant) %>% bind_cols(mu=mu)
y_pred_func <- function(x) {
delta_x <- -Inf
y_pred <- NULL
for(k in groups) {
mu_k <- data.matrix(lda_model %>% filter(groups==k) %>% dplyr::select(-groups) %>% dplyr::select(-constant))
delta_x_k <- x %*% covar_inv %*% t(mu_k) - (lda_model %>% filter(groups==k) %>% pull(constant))
if(delta_x_k > delta_x) {
y_pred <- k
delta_x <- delta_x_k
}
}
return(y_pred)
}
y_pred <- apply(validation_df %>% dplyr::select(-y), 1, y_pred_func)
return(y_pred)
}
lda_prediction <- lda_func(training_data, validation_data)
t = table(actual = validation_data$y, predict = lda_prediction)
cat("Proportion of correctly classified observations for LDA : ", (t[1,1]+t[2,2])/sum(t)*100, "%")
# Batch and Stochastic Regression errors
find_errors_regression <- function(beta, validation_df){
beta = beta[-1 * length(beta)]
df_x = cbind(x0=1, validation_df[, c(1:(ncol(validation_df) - 1))])
prob = 1 / (1 + exp(-1 * (as.matrix(df_x) %*% cbind(beta))))
y_pred = rbinom(50, 1, prob)
t = table(actual=validation_df$y, predict=y_pred)
print(t)
correct = (t[1,1]+t[2,2]) / sum(t)
return(1 - correct)
}
# LDA errors
find_errors_lda <- function(validation_data, lda_prediction){
t <- table(actual = validation_data$y, predict = lda_prediction)
print(t)
correct <- (t[1,1]+t[2,2])/sum(t)
return(1-correct)
}
# Parts of below code are from Guo & Gundogdu's HW2 solution
N <- 200
# Chosen from the graphs in the previous question
batch_learning_rate <- 1.8
stoch_learning_rate  <- 1.5
batch_error <- rep(0, N);
stoch_error <- rep(0, N);
lda_error <- rep(0, N);
for (i in 1:5)
{
batch_error[i] <- find_errors_regression(batch_func(batch_learning_rate, training_data), validation_data)
stoch_error[i] <- find_errors_regression(stochastic_func(stoch_learning_rate, training_data), validation_data)
lda_error[i] <- find_errors_lda(validation_data, lda_func(training_data, validation_data))
}
lda_func <- function(train_df, validation_df) {
groups <- unique(train_df$y)
mu <- data_frame()
constant <- c()
covar_inv <- solve(cov(train_df %>% dplyr::select(-y)))
for(k in groups) {
train_df_k <- train_df %>% filter(y==k) %>% dplyr::select(-y)
mu_k <- apply(train_df_k, 2, mean)
mu <- mu %>% bind_rows(data.frame(as.list(mu_k)))
pi_k <-nrow(train_df_k) / nrow(train_df)
constant_k <- (0.5 * t(mu_k) %*% covar_inv %*% matrix(mu_k)) + log(pi_k)
constant <- c(constant, constant_k)
}
lda_model <- data.frame(groups = groups, constant = constant) %>% bind_cols(mu=mu)
y_pred_func <- function(x) {
delta_x <- -Inf
y_pred <- NULL
for(k in groups) {
mu_k <- data.matrix(lda_model %>% filter(groups==k) %>% dplyr::select(-groups) %>% dplyr::select(-constant))
delta_x_k <- x %*% covar_inv %*% t(mu_k) - (lda_model %>% filter(groups==k) %>% pull(constant))
if(delta_x_k > delta_x) {
y_pred <- k
delta_x <- delta_x_k
}
}
return(y_pred)
}
y_pred <- apply(validation_df %>% dplyr::select(-y), 1, y_pred_func)
return(y_pred)
}
lda_prediction <- lda_func(training_data, validation_data)
t = table(actual = validation_data$y, predict = lda_prediction)
cat("Proportion of correctly classified observations for LDA : ", (t[1,1]+t[2,2])/sum(t)*100, "%")
# Batch and Stochastic Regression errors
find_errors_regression <- function(beta, validation_df){
beta = beta[-1 * length(beta)]
df_x = cbind(x0=1, validation_df[, c(1:(ncol(validation_df) - 1))])
prob = 1 / (1 + exp(-1 * (as.matrix(df_x) %*% cbind(beta))))
y_pred = rbinom(50, 1, prob)
t = table(actual=validation_df$y, predict=y_pred)
print(t)
correct = (t[1,1]+t[2,2]) / sum(t)
return(1 - correct)
}
# LDA errors
find_errors_lda <- function(validation_data, lda_prediction){
t <- table(actual = validation_data$y, predict = lda_prediction)
print(t)
correct <- (t[1,1]+t[2,2])/sum(t)
return(1-correct)
}
# Parts of below code are from Guo & Gundogdu's HW2 solution
N <- 200
# Chosen from the graphs in the previous question
batch_learning_rate <- 1.8
stoch_learning_rate  <- 1.5
batch_error <- rep(0, N);
stoch_error <- rep(0, N);
lda_error <- rep(0, N);
for (i in 1:5)
{
batch_error[i] <- find_errors_regression(batch_func(batch_learning_rate, training_data), validation_data)
stoch_error[i] <- find_errors_regression(stochastic_func(stoch_learning_rate, training_data), validation_data)
lda_error[i] <- find_errors_lda(validation_data, lda_func(training_data, validation_data))
}
par(mfrow=c(1, 3))
hist(batch_error, xlim=c(2.4,3.6))
hist(stoch_error, xlim=c(2.4,3.6))
hist(lda_error, xlim=c(2.4,3.6))
batch_error
lda_error
# Parts of below code are from Guo & Gundogdu's HW2 solution
N <- 200
# Chosen from the graphs in the previous question
batch_learning_rate <- 1.8
stoch_learning_rate  <- 1.5
batch_error <- rep(0, N);
stoch_error <- rep(0, N);
lda_error <- rep(0, N);
for (i in 1:N)
{
batch_error[i] <- find_errors_regression(batch_func(batch_learning_rate, training_data), validation_data)
stoch_error[i] <- find_errors_regression(stochastic_func(stoch_learning_rate, training_data), validation_data)
lda_error[i] <- find_errors_lda(validation_data, lda_func(training_data, validation_data))
}
diag(matrix(rep(1,4), 2 2))
diag(matrix(rep(1,4), 2, 2))
